---
sidebar: sidebar 
permalink: ai/vector-database-use-cases.html 
keywords: vector database 
summary: caso de uso - solução de banco de dados vetorial para NetApp 
---
= Casos de uso de banco de dados vetoriais
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../media/


[role="lead"]
Esta seção fornece uma visão geral dos casos de uso da solução de banco de dados vetorial NetApp.



== Casos de uso de banco de dados vetoriais

Nesta seção, discutimos sobre dois casos de uso, como a geração aumentada de recuperação com modelos de linguagem Grande e o chatbot NetApp IT.



=== Geração aumentada de recuperação (RAG) com modelos de linguagem Grande (LLMs)

....
Retrieval-augmented generation, or RAG, is a technique for enhancing the accuracy and reliability of Large Language Models, or LLMs, by augmenting prompts with facts fetched from external sources. In a traditional RAG deployment, vector embeddings are generated from an existing dataset and then stored in a vector database, often referred to as a knowledgebase. Whenever a user submits a prompt to the LLM, a vector embedding representation of the prompt is generated, and the vector database is searched using that embedding as the search query. This search operation returns similar vectors from the knowledgebase, which are then fed to the LLM as context alongside the original user prompt. In this way, an LLM can be augmented with additional information that was not part of its original training dataset.
....
O Operador LLM NVIDIA é uma ferramenta útil para implementar RAG na empresa. Esse operador pode ser usado para implantar um pipeline RAG completo. O pipeline RAG pode ser personalizado para utilizar Milvus ou pgvecto como o banco de dados vetorial para armazenar incorporações da base de dados de conhecimento. Consulte a documentação para obter detalhes.

....
NetApp has validated an enterprise RAG architecture powered by the NVIDIA Enterprise RAG LLM Operator alongside NetApp storage. Refer to our blog post for more information and to see a demo. Figure 1 provides an overview of this architecture.
....
Figura 1) RAG empresarial alimentado por microsserviços Nemo NVIDIA e NetApp

image:RAG_nvidia_nemo.png["Figura que mostra a caixa de diálogo de entrada/saída ou que representa o conteúdo escrito"]



=== Caso de uso do chatbot do NetApp IT

O chatbot do NetApp serve como outro caso de uso em tempo real para o banco de dados vetorial. Neste caso, o Sandbox OpenAI Privado do NetApp fornece uma plataforma eficaz, segura e eficiente para gerenciar consultas de usuários internos do NetApp. Ao incorporar protocolos de segurança rigorosos, sistemas de gerenciamento de dados eficientes e recursos sofisticados de processamento de AI, ele garante respostas precisas e de alta qualidade aos usuários com base em suas funções e responsabilidades na organização por meio da autenticação SSO. Essa arquitetura destaca o potencial de mesclar tecnologias avançadas para criar sistemas inteligentes e focados no usuário.

image:netapp_chatbot.png["Figura que mostra a caixa de diálogo de entrada/saída ou que representa o conteúdo escrito"]

O caso de uso pode ser dividido em quatro seções primárias.



==== Autenticação e verificação do usuário:

* As consultas do usuário primeiro passam pelo processo de logon único (SSO) do NetApp para confirmar a identidade do usuário.
* Após a autenticação bem-sucedida, o sistema verifica a conexão VPN para garantir uma transmissão segura de dados.




==== Transmissão e processamento de dados:

* Uma vez que a VPN é validada, os dados são enviados para o MariaDB através dos aplicativos web NetAIChat ou NetAICIreate. MariaDB é um sistema de banco de dados rápido e eficiente usado para gerenciar e armazenar dados do usuário.
* Em seguida, o MariaDB envia as informações para a instância do NetApp Azure, que coneta os dados do usuário à unidade de processamento de AI.




==== Interação com OpenAI e filtragem de conteúdo:

* A instância do Azure envia as perguntas do usuário para um sistema de filtragem de conteúdo. Este sistema limpa a consulta e prepara-a para processamento.
* A entrada de limpeza é então enviada para o modelo base do Azure OpenAI, que gera uma resposta baseada na entrada.




==== Geração de resposta e moderação:

* A resposta do modelo base é primeiro verificada para garantir que ele é preciso e atende aos padrões de conteúdo.
* Depois de passar a verificação, a resposta é enviada de volta ao usuário. Esse processo garante que o usuário receba uma resposta clara, precisa e apropriada para sua consulta.

