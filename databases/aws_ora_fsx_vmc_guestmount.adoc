---
sidebar: sidebar 
permalink: databases/aws_ora_fsx_vmc_guestmount.html 
keywords: Database, Oracle, AWS, FSx ONTAP, VMC, VMware 
summary: A solução fornece visão geral e detalhes para a implantação e proteção do Oracle no VMware Cloud na AWS com o FSX ONTAP como armazenamento de banco de dados principal e banco de dados Oracle configurado em reinicialização autônoma usando ASM como gerenciador de volume. 
---
= TR-4979: Oracle simplificado e autogerenciado no VMware Cloud on AWS com o FSX ONTAP instalado no convidado
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../media/


NetApp

[role="lead"]
Essa solução fornece visão geral e detalhes para a implantação e proteção do Oracle no VMware Cloud na AWS com o FSX ONTAP como armazenamento de banco de dados principal e banco de dados Oracle configurado em reinicialização autônoma usando ASM como gerenciador de volume.



== Finalidade

As empresas executam o Oracle em VMware em data centers privados há décadas. O VMware Cloud (VMC) na AWS fornece uma solução de botão para levar o software de data center definido por software (SDDC) de classe empresarial da VMware para a infraestrutura dedicada, elástica e bare-metal da nuvem AWS. O AWS FSX ONTAP oferece armazenamento premium para VMC SDDC e um Data Fabric que permite que os clientes executem aplicativos essenciais aos negócios, como o Oracle, em ambientes de nuvem privada, pública e híbrida baseados no vSphere, com acesso otimizado aos serviços da AWS. Quer se trate de um workload Oracle novo ou existente, o VMC na AWS fornece um ambiente Oracle familiar, simplificado e autogerenciado no VMware, com todos os benefícios da nuvem AWS, ao mesmo tempo em que adia todo o gerenciamento e otimização da plataforma para a VMware.

Esta documentação demonstra a implantação e proteção de um banco de dados Oracle em um ambiente VMC com o Amazon FSX ONTAP como armazenamento de banco de dados principal. O banco de dados Oracle pode ser implantado no VMC no armazenamento FSX como LUNs diretos montados no convidado da VM ou discos de armazenamento de dados VMware VMDK montados em NFS. Este relatório técnico se concentra na implantação de banco de dados Oracle como armazenamento FSX direto para VMs no cluster VMC com o protocolo iSCSI e Oracle ASM. Também demonstramos como usar a ferramenta de IU do NetApp SnapCenter para fazer backup, restaurar e clonar um banco de dados Oracle para desenvolvimento/teste ou outros casos de uso para operação de banco de dados eficiente de storage no VMC na AWS.

Esta solução aborda os seguintes casos de uso:

* Implantação de banco de dados Oracle no VMC na AWS com o Amazon FSX ONTAP como armazenamento de banco de dados principal
* Backup e restauração de banco de dados Oracle no VMC na AWS usando a ferramenta NetApp SnapCenter
* Clone de banco de dados Oracle para desenvolvimento/teste ou outros casos de uso no VMC na AWS usando a ferramenta NetApp SnapCenter




== Público-alvo

Esta solução destina-se às seguintes pessoas:

* Um DBA que gostaria de implantar o Oracle no VMC na AWS com o Amazon FSX ONTAP
* Um arquiteto de solução de banco de dados que gostaria de testar workloads Oracle no VMC na nuvem da AWS
* Um administrador de armazenamento que gostaria de implantar e gerenciar um banco de dados Oracle implantado no VMC na AWS com o Amazon FSX ONTAP
* Proprietário de um aplicativo que gostaria de levantar um banco de dados Oracle no VMC na nuvem da AWS




== Ambiente de teste e validação de soluções

O teste e a validação dessa solução foram realizados em um ambiente de laboratório com VMC na AWS que pode não corresponder ao ambiente de implantação final. Para obter mais informações, consulte a <<Fatores-chave para consideração da implantação>>seção .



=== Arquitetura

image:aws_ora_fsx_vmc_architecture.png["Esta imagem fornece uma imagem detalhada da configuração de implantação do Oracle na nuvem pública da AWS com iSCSI e ASM."]



=== Componentes de hardware e software

[cols="33%, 33%, 33%"]
|===


3+| *Hardware* 


| FSX ONTAP armazenamento | Versão atual oferecida pela AWS | Um cluster do FSX ONTAP HA na mesma VPC e zona de disponibilidade do VMC 


| Cluster VMC SDDC | Amazon EC2 i3.metal único nó/CPU Intel Xeon E5-2686, 36 núcleos/512G GB de RAM | Armazenamento VSAN de 10,37 TB 


3+| *Software* 


| RedHat Linux | RHEL-8,6, kernel 4.18.0-372,9.1.el8.x86_64 | Implantou a assinatura RedHat para testes 


| Windows Server | 2022 Standard, 10.0.20348 build 20348 | Hosting SnapCenter Server 


| Oracle Grid Infrastructure | Versão 19,18 | Aplicado patch RU p34762026_190000_Linux-x86-64.zip 


| Banco de dados Oracle | Versão 19,18 | Aplicado patch RU p34765931_190000_Linux-x86-64.zip 


| Oracle OPatch | Versão 12.2.0.1.36 | Último patch p6880880_190000_Linux-x86-64.zip 


| Servidor SnapCenter | Versão 4.9P1 | Implantação de grupo de trabalho 


| Backup e recuperação do BlueXP  para VMs | Solte 1,0 | Implantado como uma VM do plugin OVA vSphere 


| VMware vSphere | Versão 8.0.1.00300 | VMware Tools, versão: 11365 - Linux, 12352 - Windows 


| Abra o JDK | Versão Java-1,8.0-openjdk.x86_64 | Requisito de plug-in do SnapCenter em VMs de banco de dados 
|===


=== Configuração de banco de dados Oracle no VMC na AWS

[cols="33%, 33%, 33%"]
|===


3+|  


| *Servidor* | *Base de dados* | *DB Storage* 


| ora_01 | cdb1 (cdb1_pdb1,cdb1_pdb2,cdb1_pdb3) | Armazenamento de dados VMDK no FSX ONTAP 


| ora_01 | cdb2 (cdb2_pdb) | Armazenamento de dados VMDK no FSX ONTAP 


| ora_02 | cdb3 (cdb3_pdb1,cdb3_pdb2,cdb3_pdb3) | O FSX ONTAP foi montado diretamente pelo hóspede 


| ora_02 | cdb4 (cdb4_pdb) | O FSX ONTAP foi montado diretamente pelo hóspede 
|===


=== Fatores-chave para consideração da implantação

* *FSX para conetividade VMC.* Ao implantar o SDDC no VMware Cloud na AWS, ele é criado em uma conta da AWS e em uma VPC dedicada à sua organização e gerenciada pela VMware. Você também deve conetar o SDDC a uma conta da AWS que pertence a você, chamada de conta AWS cliente. Essa conexão permite que o SDDC acesse serviços da AWS pertencentes à sua conta de cliente. O FSX ONTAP é um serviço da AWS implantado na sua conta de cliente. Uma vez que o VMC SDDC é conetado à sua conta de cliente, o armazenamento FSX está disponível para VMs no VMC SDDC para montagem direta de convidados.
* *FSX storage HA clusters implantação de uma ou várias zonas.* Nesses testes e validações, implantamos um cluster do FSX HA em uma única zona de disponibilidade da AWS. A NetApp também recomenda implantar o FSX ONTAP e o VMware Cloud na AWS na mesma zona de disponibilidade para obter melhor desempenho e evitar cobranças de transferência de dados entre zonas de disponibilidade.
* * Dimensionamento de cluster de armazenamento FSX.* Um sistema de arquivos de armazenamento Amazon FSX ONTAP oferece até 160.000 IOPS SSD bruto, taxa de transferência de até 4Gbps Gbps e capacidade máxima de 192TiB TB. No entanto, você pode dimensionar o cluster em termos de IOPS provisionadas, taxa de transferência e limite de storage (mínimo de 1.024 GiB) com base em seus requisitos reais no momento da implantação. A capacidade pode ser ajustada dinamicamente em tempo real, sem afetar a disponibilidade da aplicação.
* * Layout de dados e logs do Oracle.* Em nossos testes e validações, implantamos dois grupos de discos ASM para dados e logs, respetivamente. Dentro do grupo de discos de mais de um volume de dados, provisionamos quatro LUNs em um volume de dados. Dentro do grupo de discos ASM de LOGS, nós provisionamos dois LUNs em um volume de log. Em geral, vários LUNs dispostos em um volume do Amazon FSX ONTAP oferecem melhor desempenho.
* *Configuração iSCSI.* As VMs de banco de dados no VMC SDDC se conetam ao armazenamento FSX com o protocolo iSCSI. É importante avaliar o requisito de taxa de transferência de e/S de pico do banco de dados Oracle, analisando cuidadosamente o relatório AWR do Oracle para determinar os requisitos de taxa de transferência de tráfego de aplicativos e iSCSI. A NetApp também recomenda alocar quatro conexões iSCSI para ambos os pontos de extremidade iSCSI do FSX com multipath devidamente configurado.
* *Nível de redundância Oracle ASM para usar para cada grupo de discos Oracle ASM que você criar.* Como o FSX ONTAP já espelha o armazenamento no nível do cluster do FSX, você deve usar redundância externa, o que significa que a opção não permite que o Oracle ASM espelhe o conteúdo do grupo de discos.
* *Backup do banco de dados.* O NetApp fornece um pacote de software SnapCenter para backup, restauração e clonagem de banco de dados com uma interface de usuário amigável. A NetApp recomenda a implementação dessa ferramenta de gerenciamento para obter backup instantâneo rápido (em menos de um minuto), restauração rápida de banco de dados (em minutos) e clone de banco de dados.




== Implantação de solução

As seções a seguir fornecem procedimentos passo a passo para a implantação do Oracle 19Ci no VMC na AWS com armazenamento FSX ONTAP montado diretamente para VM DB em uma configuração de reinicialização de nó único com o Oracle ASM como gerenciador de volume de banco de dados.



=== Pré-requisitos para implantação

[%collapsible%open]
====
A implantação requer os seguintes pré-requisitos.

. Um data center definido por software (SDDC) usando o VMware Cloud na AWS foi criado. Para obter instruções detalhadas sobre como criar um SDDC no VMC, consulte a documentação da VMware link:https://docs.vmware.com/en/VMware-Cloud-on-AWS/services/com.vmware.vmc-aws.getting-started/GUID-3D741363-F66A-4CF9-80EA-AA2866D1834E.html["Primeiros passos com o VMware Cloud na AWS"^]
. Uma conta da AWS foi configurada e os segmentos de rede e VPC necessários foram criados na sua conta da AWS. A conta da AWS está vinculada ao seu VMC SDDC.
. No console do AWS EC2, implantando clusters de HA de armazenamento do Amazon FSX ONTAP para hospedar volumes de banco de dados Oracle. Se você não estiver familiarizado com a implantação do FSX storage, consulte a documentação link:https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/creating-file-systems.html["Criando sistemas de arquivos FSX ONTAP"^] para obter instruções passo a passo.
. A etapa acima pode ser executada usando o seguinte kit de ferramentas de automação Terraform, que cria uma instância EC2 como um host de salto para SDDC no acesso VMC via SSH e um sistema de arquivos FSX. Revise as instruções cuidadosamente e altere as variáveis para se adequar ao seu ambiente antes da execução.
+
....
git clone https://github.com/NetApp-Automation/na_aws_fsx_ec2_deploy.git
....
. Crie VMs no VMware SDDC na AWS para hospedar seu ambiente Oracle a ser implantado no VMC. Em nossa demonstração, criamos duas VMs Linux como servidores Oracle DB, um servidor Windows para o servidor SnapCenter e um servidor Linux opcional como controlador Ansible para instalação ou configuração automatizada Oracle, se desejado. A seguir está um snapshot do ambiente de laboratório para a validação da solução.
+
image:aws_ora_fsx_vmc_vm_08.png["Captura de tela mostrando o ambiente de teste do VMC SDDC."]

. Opcionalmente, o NetApp também fornece vários kits de ferramentas de automação para executar a implantação e configuração do Oracle quando aplicável. link:index.html["Kits de ferramentas de automação DB"^]Consulte para obter mais informações.



NOTE: Certifique-se de ter alocado pelo MENOS 50g GB no volume raiz da VM Oracle para ter espaço suficiente para preparar os arquivos de instalação Oracle.

====


=== Configuração do kernel DB VM

[%collapsible%open]
====
Com os pré-requisitos provisionados, faça login na VM Oracle como um usuário admin via SSH e faça o sudo para o usuário root para configurar o kernel Linux para instalação Oracle. Os arquivos de instalação do Oracle podem ser colocados em um bucket do AWS S3 e transferidos para a VM.

. Crie uma pasta de diretório de preparação `/tmp/archive` e defina a `777` permissão.
+
[source, cli]
----
mkdir /tmp/archive
----
+
[source, cli]
----
chmod 777 /tmp/archive
----
. Baixe e coloque os arquivos de instalação binários Oracle e outros arquivos rpm necessários para o `/tmp/archive` diretório.
+
Veja a seguinte lista de arquivos de instalação a serem indicados na `/tmp/archive` VM DB.

+
....

[admin@ora_02 ~]$ ls -l /tmp/archive/
total 10539364
-rw-rw-r--. 1 admin  admin         19112 Oct  4 17:04 compat-libcap1-1.10-7.el7.x86_64.rpm
-rw-rw-r--. 1 admin  admin    3059705302 Oct  4 17:10 LINUX.X64_193000_db_home.zip
-rw-rw-r--. 1 admin  admin    2889184573 Oct  4 17:11 LINUX.X64_193000_grid_home.zip
-rw-rw-r--. 1 admin  admin        589145 Oct  4 17:04 netapp_linux_unified_host_utilities-7-1.x86_64.rpm
-rw-rw-r--. 1 admin  admin         31828 Oct  4 17:04 oracle-database-preinstall-19c-1.0-2.el8.x86_64.rpm
-rw-rw-r--. 1 admin  admin    2872741741 Oct  4 17:12 p34762026_190000_Linux-x86-64.zip
-rw-rw-r--. 1 admin  admin    1843577895 Oct  4 17:13 p34765931_190000_Linux-x86-64.zip
-rw-rw-r--. 1 admin  admin     124347218 Oct  4 17:13 p6880880_190000_Linux-x86-64.zip
-rw-rw-r--. 1 admin  admin        257136 Oct  4 17:04 policycoreutils-python-utils-2.9-9.el8.noarch.rpm
[admin@ora_02 ~]$

....
. Instale o Oracle 19C pré-instalação RPM, que satisfaz a maioria dos requisitos de configuração do kernel.
+
[source, cli]
----
yum install /tmp/archive/oracle-database-preinstall-19c-1.0-2.el8.x86_64.rpm
----
. Baixe e instale o que está faltando `compat-libcap1` no Linux 8.
+
[source, cli]
----
yum install /tmp/archive/compat-libcap1-1.10-7.el7.x86_64.rpm
----
. A partir do NetApp, baixe e instale os utilitários de host do NetApp.
+
[source, cli]
----
yum install /tmp/archive/netapp_linux_unified_host_utilities-7-1.x86_64.rpm
----
. Instale `policycoreutils-python-utils`o .
+
[source, cli]
----
yum install /tmp/archive/policycoreutils-python-utils-2.9-9.el8.noarch.rpm
----
. Instale o Open JDK versão 1,8.
+
[source, cli]
----
yum install java-1.8.0-openjdk.x86_64
----
. Instale utilitários do iniciador iSCSI.
+
[source, cli]
----
yum install iscsi-initiator-utils
----
. Instale o sg3_utils.
+
[source, cli]
----
yum install sg3_utils
----
. Instale o Device-mapper-multipath.
+
[source, cli]
----
yum install device-mapper-multipath
----
. Desative os hugepages transparentes no sistema atual.
+
[source, cli]
----
echo never > /sys/kernel/mm/transparent_hugepage/enabled
----
+
[source, cli]
----
echo never > /sys/kernel/mm/transparent_hugepage/defrag
----
. Adicione as seguintes linhas `/etc/rc.local` para desativar `transparent_hugepage` após a reinicialização.
+
[source, cli]
----
vi /etc/rc.local
----
+
....
  # Disable transparent hugepages
          if test -f /sys/kernel/mm/transparent_hugepage/enabled; then
            echo never > /sys/kernel/mm/transparent_hugepage/enabled
          fi
          if test -f /sys/kernel/mm/transparent_hugepage/defrag; then
            echo never > /sys/kernel/mm/transparent_hugepage/defrag
          fi
....
. Desative o selinux alterando `SELINUX=enforcing` para `SELINUX=disabled`. Você deve reiniciar o host para tornar a alteração efetiva.
+
[source, cli]
----
vi /etc/sysconfig/selinux
----
. Adicione as linhas a seguir `limit.conf` para definir o limite do descritor de arquivo e o tamanho da pilha.
+
[source, cli]
----
vi /etc/security/limits.conf
----
+
....

*               hard    nofile          65536
*               soft    stack           10240
....
. Adicione espaço de swap à VM DB se não houver espaço de swap configurado com esta instrução: link:https://aws.amazon.com/premiumsupport/knowledge-center/ec2-memory-swap-file/["Como alocar memória para funcionar como espaço de troca em uma instância do Amazon EC2 usando um arquivo de swap?"^] A quantidade exata de espaço a adicionar depende do tamanho da RAM até 16GGB.
. Altere o `node.session.timeo.replacement_timeout` `iscsi.conf` arquivo de configuração de 120 para 5 segundos.
+
[source, cli]
----
vi /etc/iscsi/iscsid.conf
----
. Ative e inicie o serviço iSCSI na instância EC2.
+
[source, cli]
----
systemctl enable iscsid
----
+
[source, cli]
----
systemctl start iscsid
----
. Recupere o endereço do iniciador iSCSI a ser utilizado para o mapeamento LUN da base de dados.
+
[source, cli]
----
cat /etc/iscsi/initiatorname.iscsi
----
. Adicione os grupos ASM para o usuário de gerenciamento ASM (oracle).
+
[source, cli]
----
groupadd asmadmin
----
+
[source, cli]
----
groupadd asmdba
----
+
[source, cli]
----
groupadd asmoper
----
. Modifique o usuário oracle para adicionar grupos ASM como grupos secundários (o usuário oracle deve ter sido criado após a instalação do Oracle pré-instalar RPM).
+
[source, cli]
----
usermod -a -G asmadmin oracle
----
+
[source, cli]
----
usermod -a -G asmdba oracle
----
+
[source, cli]
----
usermod -a -G asmoper oracle
----
. Pare e desative o firewall do Linux se ele estiver ativo.
+
[source, cli]
----
systemctl stop firewalld
----
+
[source, cli]
----
systemctl disable firewalld
----
. Ative o sudo sem senha para o usuário admin descomentando `# %wheel  ALL=(ALL)       NOPASSWD: ALL` a linha no arquivo /etc/sudoers. Altere a permissão de arquivo para fazer a edição.
+
[source, cli]
----
chmod 640 /etc/sudoers
----
+
[source, cli]
----
vi /etc/sudoers
----
+
[source, cli]
----
chmod 440 /etc/sudoers
----
. Reinicie a instância EC2.


====


=== Provisione e mapeie LUNs do FSX ONTAP para a VM de banco de dados

[%collapsible%open]
====
Provisione três volumes da linha de comando fazendo login no cluster FSX como usuário fsxadmin via ssh e IP de gerenciamento de cluster FSX. Crie LUNs nos volumes para hospedar os arquivos binários, dados e logs do banco de dados Oracle.

. Faça login no cluster FSX através do SSH como o usuário fsxadmin.
+
[source, cli]
----
ssh fsxadmin@10.49.0.74
----
. Execute o seguinte comando para criar um volume para o binário Oracle.
+
[source, cli]
----
vol create -volume ora_02_biny -aggregate aggr1 -size 50G -state online  -type RW -snapshot-policy none -tiering-policy snapshot-only
----
. Execute o seguinte comando para criar um volume para dados Oracle.
+
[source, cli]
----
vol create -volume ora_02_data -aggregate aggr1 -size 100G -state online  -type RW -snapshot-policy none -tiering-policy snapshot-only
----
. Execute o seguinte comando para criar um volume para logs Oracle.
+
[source, cli]
----
vol create -volume ora_02_logs -aggregate aggr1 -size 100G -state online  -type RW -snapshot-policy none -tiering-policy snapshot-only
----
. Valide os volumes criados.
+
[source, cli]
----
vol show ora*
----
+
Saída do comando:

+
....
FsxId0c00cec8dad373fd1::> vol show ora*
Vserver   Volume       Aggregate    State      Type       Size  Available Used%
--------- ------------ ------------ ---------- ---- ---------- ---------- -----
nim       ora_02_biny  aggr1        online     RW         50GB    22.98GB   51%
nim       ora_02_data  aggr1        online     RW        100GB    18.53GB   80%
nim       ora_02_logs  aggr1        online     RW         50GB     7.98GB   83%
....
. Crie um LUN binário dentro do volume binário do banco de dados.
+
[source, cli]
----
lun create -path /vol/ora_02_biny/ora_02_biny_01 -size 40G -ostype linux
----
. Criar LUNs de dados no volume de dados do banco de dados.
+
[source, cli]
----
lun create -path /vol/ora_02_data/ora_02_data_01 -size 20G -ostype linux
----
+
[source, cli]
----
lun create -path /vol/ora_02_data/ora_02_data_02 -size 20G -ostype linux
----
+
[source, cli]
----
lun create -path /vol/ora_02_data/ora_02_data_03 -size 20G -ostype linux
----
+
[source, cli]
----
lun create -path /vol/ora_02_data/ora_02_data_04 -size 20G -ostype linux
----
. Crie LUNs de log dentro do volume de logs do banco de dados.
+
[source, cli]
----
lun create -path /vol/ora_02_logs/ora_02_logs_01 -size 40G -ostype linux
----
+
[source, cli]
----
lun create -path /vol/ora_02_logs/ora_02_logs_02 -size 40G -ostype linux
----
. Crie um grupo para a instância EC2 com o iniciador recuperado da etapa 14 da configuração do kernel EC2 acima.
+
[source, cli]
----
igroup create -igroup ora_02 -protocol iscsi -ostype linux -initiator iqn.1994-05.com.redhat:f65fed7641c2
----
. Mapeie os LUNs para o grupo criado acima. Incremente a ID LUN sequencialmente para cada LUN adicional.
+
[source, cli]
----
lun map -path /vol/ora_02_biny/ora_02_biny_01 -igroup ora_02 -vserver svm_ora -lun-id 0
lun map -path /vol/ora_02_data/ora_02_data_01 -igroup ora_02 -vserver svm_ora -lun-id 1
lun map -path /vol/ora_02_data/ora_02_data_02 -igroup ora_02 -vserver svm_ora -lun-id 2
lun map -path /vol/ora_02_data/ora_02_data_03 -igroup ora_02 -vserver svm_ora -lun-id 3
lun map -path /vol/ora_02_data/ora_02_data_04 -igroup ora_02 -vserver svm_ora -lun-id 4
lun map -path /vol/ora_02_logs/ora_02_logs_01 -igroup ora_02 -vserver svm_ora -lun-id 5
lun map -path /vol/ora_02_logs/ora_02_logs_02 -igroup ora_02 -vserver svm_ora -lun-id 6
----
. Valide o mapeamento LUN.
+
[source, cli]
----
mapping show
----
+
Espera-se que isso retorne:

+
....
FsxId0c00cec8dad373fd1::> mapping show
  (lun mapping show)
Vserver    Path                                      Igroup   LUN ID  Protocol
---------- ----------------------------------------  -------  ------  --------
nim        /vol/ora_02_biny/ora_02_u01_01            ora_02        0  iscsi
nim        /vol/ora_02_data/ora_02_u02_01            ora_02        1  iscsi
nim        /vol/ora_02_data/ora_02_u02_02            ora_02        2  iscsi
nim        /vol/ora_02_data/ora_02_u02_03            ora_02        3  iscsi
nim        /vol/ora_02_data/ora_02_u02_04            ora_02        4  iscsi
nim        /vol/ora_02_logs/ora_02_u03_01            ora_02        5  iscsi
nim        /vol/ora_02_logs/ora_02_u03_02            ora_02        6  iscsi
....


====


=== Configuração de armazenamento DB VM

[%collapsible%open]
====
Agora, importe e configure o armazenamento FSX ONTAP para a instalação de banco de dados e infraestrutura de grade Oracle na VM de banco de dados VMC.

. Faça login na VM do banco de dados via SSH como o usuário administrativo usando o Putty do servidor de saltos do Windows.
. Descubra os pontos de extremidade iSCSI FSX usando o endereço IP iSCSI SVM. Mude para o endereço do portal específico do ambiente.
+
[source, cli]
----
sudo iscsiadm iscsiadm --mode discovery --op update --type sendtargets --portal 10.49.0.12
----
. Estabeleça sessões iSCSI efetuando login em cada destino.
+
[source, cli]
----
sudo iscsiadm --mode node -l all
----
+
A saída esperada do comando é:

+
....
[ec2-user@ip-172-30-15-58 ~]$ sudo iscsiadm --mode node -l all
Logging in to [iface: default, target: iqn.1992-08.com.netapp:sn.1f795e65c74911edb785affbf0a2b26e:vs.3, portal: 10.49.0.12,3260]
Logging in to [iface: default, target: iqn.1992-08.com.netapp:sn.1f795e65c74911edb785affbf0a2b26e:vs.3, portal: 10.49.0.186,3260]
Login to [iface: default, target: iqn.1992-08.com.netapp:sn.1f795e65c74911edb785affbf0a2b26e:vs.3, portal: 10.49.0.12,3260] successful.
Login to [iface: default, target: iqn.1992-08.com.netapp:sn.1f795e65c74911edb785affbf0a2b26e:vs.3, portal: 10.49.0.186,3260] successful.
....
. Visualizar e validar uma lista de sessões iSCSI ativas.
+
[source, cli]
----
sudo iscsiadm --mode session
----
+
Retornar as sessões iSCSI.

+
....
[ec2-user@ip-172-30-15-58 ~]$ sudo iscsiadm --mode session
tcp: [1] 10.49.0.186:3260,1028 iqn.1992-08.com.netapp:sn.545a38bf06ac11ee8503e395ab90d704:vs.3 (non-flash)
tcp: [2] 10.49.0.12:3260,1029 iqn.1992-08.com.netapp:sn.545a38bf06ac11ee8503e395ab90d704:vs.3 (non-flash)
....
. Verifique se os LUNs foram importados para o host.
+
[source, cli]
----
sudo sanlun lun show
----
+
Isso retornará uma lista de LUNs Oracle do FSX.

+
....

[admin@ora_02 ~]$ sudo sanlun lun show
controller(7mode/E-Series)/                                                  device          host                  lun
vserver(cDOT/FlashRay)        lun-pathname                                   filename        adapter    protocol   size    product
-------------------------------------------------------------------------------------------------------------------------------
nim                           /vol/ora_02_logs/ora_02_u03_02                 /dev/sdo        host34     iSCSI      20g     cDOT
nim                           /vol/ora_02_logs/ora_02_u03_01                 /dev/sdn        host34     iSCSI      20g     cDOT
nim                           /vol/ora_02_data/ora_02_u02_04                 /dev/sdm        host34     iSCSI      20g     cDOT
nim                           /vol/ora_02_data/ora_02_u02_03                 /dev/sdl        host34     iSCSI      20g     cDOT
nim                           /vol/ora_02_data/ora_02_u02_02                 /dev/sdk        host34     iSCSI      20g     cDOT
nim                           /vol/ora_02_data/ora_02_u02_01                 /dev/sdj        host34     iSCSI      20g     cDOT
nim                           /vol/ora_02_biny/ora_02_u01_01                 /dev/sdi        host34     iSCSI      40g     cDOT
nim                           /vol/ora_02_logs/ora_02_u03_02                 /dev/sdh        host33     iSCSI      20g     cDOT
nim                           /vol/ora_02_logs/ora_02_u03_01                 /dev/sdg        host33     iSCSI      20g     cDOT
nim                           /vol/ora_02_data/ora_02_u02_04                 /dev/sdf        host33     iSCSI      20g     cDOT
nim                           /vol/ora_02_data/ora_02_u02_03                 /dev/sde        host33     iSCSI      20g     cDOT
nim                           /vol/ora_02_data/ora_02_u02_02                 /dev/sdd        host33     iSCSI      20g     cDOT
nim                           /vol/ora_02_data/ora_02_u02_01                 /dev/sdc        host33     iSCSI      20g     cDOT
nim                           /vol/ora_02_biny/ora_02_u01_01                 /dev/sdb        host33     iSCSI      40g     cDOT

....
. Configure o `multipath.conf` arquivo com as seguintes entradas padrão e blacklist.
+
[source, cli]
----
sudo vi /etc/multipath.conf
----
+
Adicione as seguintes entradas:

+
....
defaults {
    find_multipaths yes
    user_friendly_names yes
}

blacklist {
    devnode "^(ram|raw|loop|fd|md|dm-|sr|scd|st)[0-9]*"
    devnode "^hd[a-z]"
    devnode "^cciss.*"
}
....
. Inicie o serviço multipath.
+
[source, cli]
----
sudo systemctl start multipathd
----
+
Agora os dispositivos multipath aparecem `/dev/mapper` no diretório.

+
....
[ec2-user@ip-172-30-15-58 ~]$ ls -l /dev/mapper
total 0
lrwxrwxrwx 1 root root       7 Mar 21 20:13 3600a09806c574235472455534e68512d -> ../dm-0
lrwxrwxrwx 1 root root       7 Mar 21 20:13 3600a09806c574235472455534e685141 -> ../dm-1
lrwxrwxrwx 1 root root       7 Mar 21 20:13 3600a09806c574235472455534e685142 -> ../dm-2
lrwxrwxrwx 1 root root       7 Mar 21 20:13 3600a09806c574235472455534e685143 -> ../dm-3
lrwxrwxrwx 1 root root       7 Mar 21 20:13 3600a09806c574235472455534e685144 -> ../dm-4
lrwxrwxrwx 1 root root       7 Mar 21 20:13 3600a09806c574235472455534e685145 -> ../dm-5
lrwxrwxrwx 1 root root       7 Mar 21 20:13 3600a09806c574235472455534e685146 -> ../dm-6
crw------- 1 root root 10, 236 Mar 21 18:19 control
....
. Faça login no cluster do FSX ONTAP como usuário do fsxadmin via SSH para recuperar o número hex de série para cada LUN começando com 6c574xxx..., o NÚMERO HEX começa com 3600a0980, que é a ID do fornecedor da AWS.
+
[source, cli]
----
lun show -fields serial-hex
----
+
e retornar como segue:

+
....
FsxId02ad7bf3476b741df::> lun show -fields serial-hex
vserver path                            serial-hex
------- ------------------------------- ------------------------
svm_ora /vol/ora_02_biny/ora_02_biny_01 6c574235472455534e68512d
svm_ora /vol/ora_02_data/ora_02_data_01 6c574235472455534e685141
svm_ora /vol/ora_02_data/ora_02_data_02 6c574235472455534e685142
svm_ora /vol/ora_02_data/ora_02_data_03 6c574235472455534e685143
svm_ora /vol/ora_02_data/ora_02_data_04 6c574235472455534e685144
svm_ora /vol/ora_02_logs/ora_02_logs_01 6c574235472455534e685145
svm_ora /vol/ora_02_logs/ora_02_logs_02 6c574235472455534e685146
7 entries were displayed.
....
. Atualize o `/dev/multipath.conf` ficheiro para adicionar um nome fácil de utilizar para o dispositivo multipath.
+
[source, cli]
----
sudo vi /etc/multipath.conf
----
+
com as seguintes entradas:

+
....
multipaths {
        multipath {
                wwid            3600a09806c574235472455534e68512d
                alias           ora_02_biny_01
        }
        multipath {
                wwid            3600a09806c574235472455534e685141
                alias           ora_02_data_01
        }
        multipath {
                wwid            3600a09806c574235472455534e685142
                alias           ora_02_data_02
        }
        multipath {
                wwid            3600a09806c574235472455534e685143
                alias           ora_02_data_03
        }
        multipath {
                wwid            3600a09806c574235472455534e685144
                alias           ora_02_data_04
        }
        multipath {
                wwid            3600a09806c574235472455534e685145
                alias           ora_02_logs_01
        }
        multipath {
                wwid            3600a09806c574235472455534e685146
                alias           ora_02_logs_02
        }
}
....
. Reinicie o serviço multipath para verificar se os dispositivos em `/dev/mapper` foram alterados para nomes LUN versus IDs seriais-hex.
+
[source, cli]
----
sudo systemctl restart multipathd
----
+
Verifique `/dev/mapper` para retornar da seguinte forma:

+
....
[ec2-user@ip-172-30-15-58 ~]$ ls -l /dev/mapper
total 0
crw------- 1 root root 10, 236 Mar 21 18:19 control
lrwxrwxrwx 1 root root       7 Mar 21 20:41 ora_02_biny_01 -> ../dm-0
lrwxrwxrwx 1 root root       7 Mar 21 20:41 ora_02_data_01 -> ../dm-1
lrwxrwxrwx 1 root root       7 Mar 21 20:41 ora_02_data_02 -> ../dm-2
lrwxrwxrwx 1 root root       7 Mar 21 20:41 ora_02_data_03 -> ../dm-3
lrwxrwxrwx 1 root root       7 Mar 21 20:41 ora_02_data_04 -> ../dm-4
lrwxrwxrwx 1 root root       7 Mar 21 20:41 ora_02_logs_01 -> ../dm-5
lrwxrwxrwx 1 root root       7 Mar 21 20:41 ora_02_logs_02 -> ../dm-6
....
. Particione o LUN binário com uma única partição primária.
+
[source, cli]
----
sudo fdisk /dev/mapper/ora_02_biny_01
----
. Formate o LUN binário particionado com um sistema de ficheiros XFS.
+
[source, cli]
----
sudo mkfs.xfs /dev/mapper/ora_02_biny_01p1
----
. Monte o LUN binário em `/u01`.
+
[source, cli]
----
sudo mkdir /u01
----
+
[source, cli]
----
sudo mount -t xfs /dev/mapper/ora_02_biny_01p1 /u01
----
. Altere `/u01` a propriedade do ponto de montagem para o usuário oracle e seu grupo primário associado.
+
[source, cli]
----
sudo chown oracle:oinstall /u01
----
. Encontre a UUI do LUN binário.
+
[source, cli]
----
sudo blkid /dev/mapper/ora_02_biny_01p1
----
. Adicione um ponto de montagem ao `/etc/fstab`.
+
[source, cli]
----
sudo vi /etc/fstab
----
+
Adicione a seguinte linha.

+
....
UUID=d89fb1c9-4f89-4de4-b4d9-17754036d11d       /u01    xfs     defaults,nofail 0       2
....
. Como usuário raiz, adicione a regra udev para dispositivos Oracle.
+
[source, cli]
----
vi /etc/udev/rules.d/99-oracle-asmdevices.rules
----
+
Inclua as seguintes entradas:

+
....
ENV{DM_NAME}=="ora*", GROUP:="oinstall", OWNER:="oracle", MODE:="660"
....
. Como usuário root, recarregue as regras do udev.
+
[source, cli]
----
udevadm control --reload-rules
----
. Como usuário root, acione as regras do udev.
+
[source, cli]
----
udevadm trigger
----
. Como usuário raiz, recarregue multipathd.
+
[source, cli]
----
systemctl restart multipathd
----
. Reinicie o host da instância EC2.


====


=== Instalação da infraestrutura Oracle Grid

[%collapsible%open]
====
. Faça login na VM DB como usuário admin via SSH e ative a autenticação de senha descomentando `PasswordAuthentication yes` e depois comentando `PasswordAuthentication no` .
+
[source, cli]
----
sudo vi /etc/ssh/sshd_config
----
. Reinicie o serviço sshd.
+
[source, cli]
----
sudo systemctl restart sshd
----
. Redefina a senha do usuário Oracle.
+
[source, cli]
----
sudo passwd oracle
----
. Faça login como o usuário proprietário do software (oracle Restart) da Oracle. Crie um diretório Oracle da seguinte forma:
+
[source, cli]
----
mkdir -p /u01/app/oracle
----
+
[source, cli]
----
mkdir -p /u01/app/oraInventory
----
. Altere a configuração de permissão de diretório.
+
[source, cli]
----
chmod -R 775 /u01/app
----
. Crie um diretório inicial de grade e mude para ele.
+
[source, cli]
----
mkdir -p /u01/app/oracle/product/19.0.0/grid
----
+
[source, cli]
----
cd /u01/app/oracle/product/19.0.0/grid
----
. Descompacte os arquivos de instalação da grade.
+
[source, cli]
----
unzip -q /tmp/archive/LINUX.X64_193000_grid_home.zip
----
. A partir da página inicial da grelha, elimine o `OPatch` diretório.
+
[source, cli]
----
rm -rf OPatch
----
. A partir da grelha para casa, descompacte . `p6880880_190000_Linux-x86-64.zip`
+
[source, cli]
----
unzip -q /tmp/archive/p6880880_190000_Linux-x86-64.zip
----
. A partir da página inicial da grelha, rever `cv/admin/cvu_config`, descomentar e substituir `CV_ASSUME_DISTID=OEL5` por `CV_ASSUME_DISTID=OL7`.
+
[source, cli]
----
vi cv/admin/cvu_config
----
. Prepare um `gridsetup.rsp` arquivo para instalação silenciosa e coloque o arquivo rsp no `/tmp/archive` diretório. O arquivo rsp deve abranger as seções A, B e G com as seguintes informações:
+
....
INVENTORY_LOCATION=/u01/app/oraInventory
oracle.install.option=HA_CONFIG
ORACLE_BASE=/u01/app/oracle
oracle.install.asm.OSDBA=asmdba
oracle.install.asm.OSOPER=asmoper
oracle.install.asm.OSASM=asmadmin
oracle.install.asm.SYSASMPassword="SetPWD"
oracle.install.asm.diskGroup.name=DATA
oracle.install.asm.diskGroup.redundancy=EXTERNAL
oracle.install.asm.diskGroup.AUSize=4
oracle.install.asm.diskGroup.disks=/dev/mapper/ora_02_data_01,/dev/mapper/ora_02_data_02,/dev/mapper/ora_02_data_03,/dev/mapper/ora_02_data_04
oracle.install.asm.diskGroup.diskDiscoveryString=/dev/mapper/*
oracle.install.asm.monitorPassword="SetPWD"
oracle.install.asm.configureAFD=true
....
. Faça login na instância EC2 como usuário raiz e defina `ORACLE_HOME` e `ORACLE_BASE`.
+
[source, cli]
----
export ORACLE_HOME=/u01/app/oracle/product/19.0.0/
----
+
[source, cli]
----
export ORACLE_BASE=/tmp
----
+
[source, cli]
----
cd /u01/app/oracle/product/19.0.0/grid/bin
----
. Inicialize dispositivos de disco para uso com o driver de filtro Oracle ASM.
+
[source, cli]
----
 ./asmcmd afd_label DATA01 /dev/mapper/ora_02_data_01 --init
----
+
[source, cli]
----
 ./asmcmd afd_label DATA02 /dev/mapper/ora_02_data_02 --init
----
+
[source, cli]
----
 ./asmcmd afd_label DATA03 /dev/mapper/ora_02_data_03 --init
----
+
[source, cli]
----
 ./asmcmd afd_label DATA04 /dev/mapper/ora_02_data_04 --init
----
+
[source, cli]
----
 ./asmcmd afd_label LOGS01 /dev/mapper/ora_02_logs_01 --init
----
+
[source, cli]
----
 ./asmcmd afd_label LOGS02 /dev/mapper/ora_02_logs_02 --init
----
. Instale `cvuqdisk-1.0.10-1.rpm`o .
+
[source, cli]
----
rpm -ivh /u01/app/oracle/product/19.0.0/grid/cv/rpm/cvuqdisk-1.0.10-1.rpm
----
. Anular definição `$ORACLE_BASE`.
+
[source, cli]
----
unset ORACLE_BASE
----
. Faça login na instância EC2 como o usuário Oracle e extraia o patch na `/tmp/archive` pasta.
+
[source, cli]
----
unzip -q /tmp/archive/p34762026_190000_Linux-x86-64.zip -d /tmp/archive
----
. A partir da página inicial do Grid /u01/app/oracle/product/19,0.0/grid e como usuário do oracle, inicie `gridSetup.sh` a instalação da infraestrutura de grade.
+
[source, cli]
----
 ./gridSetup.sh -applyRU /tmp/archive/34762026/ -silent -responseFile /tmp/archive/gridsetup.rsp
----
. Como usuário root, execute o(s) seguinte(s) script(s):
+
[source, cli]
----
/u01/app/oraInventory/orainstRoot.sh
----
+
[source, cli]
----
/u01/app/oracle/product/19.0.0/grid/root.sh
----
. Como usuário root, recarregue o multipathd.
+
[source, cli]
----
systemctl restart multipathd
----
. Como usuário Oracle, execute o seguinte comando para concluir a configuração:
+
[source, cli]
----
/u01/app/oracle/product/19.0.0/grid/gridSetup.sh -executeConfigTools -responseFile /tmp/archive/gridsetup.rsp -silent
----
. Como usuário Oracle, crie o grupo de discos DE LOGS.
+
[source, cli]
----
bin/asmca -silent -sysAsmPassword 'yourPWD' -asmsnmpPassword 'yourPWD' -createDiskGroup -diskGroupName LOGS -disk 'AFD:LOGS*' -redundancy EXTERNAL -au_size 4
----
. Como usuário Oracle, valide os serviços de grade após a configuração da instalação.
+
[source, cli]
----
bin/crsctl stat res -t
----
+
....
[oracle@ora_02 grid]$ bin/crsctl stat res -t
--------------------------------------------------------------------------------
Name           Target  State        Server                   State details
--------------------------------------------------------------------------------
Local Resources
--------------------------------------------------------------------------------
ora.DATA.dg
               ONLINE  ONLINE       ora_02                   STABLE
ora.LISTENER.lsnr
               ONLINE  INTERMEDIATE ora_02                   Not All Endpoints Re
                                                             gistered,STABLE
ora.LOGS.dg
               ONLINE  ONLINE       ora_02                   STABLE
ora.asm
               ONLINE  ONLINE       ora_02                   Started,STABLE
ora.ons
               OFFLINE OFFLINE      ora_02                   STABLE
--------------------------------------------------------------------------------
Cluster Resources
--------------------------------------------------------------------------------
ora.cssd
      1        ONLINE  ONLINE       ora_02                   STABLE
ora.diskmon
      1        OFFLINE OFFLINE                               STABLE
ora.driver.afd
      1        ONLINE  ONLINE       ora_02                   STABLE
ora.evmd
      1        ONLINE  ONLINE       ora_02                   STABLE
--------------------------------------------------------------------------------
....
. Estado do controlador do filtro ASM do Valiate.
+
....

[oracle@ora_02 grid]$ export ORACLE_HOME=/u01/app/oracle/product/19.0.0/grid
[oracle@ora_02 grid]$ export ORACLE_SID=+ASM
[oracle@ora_02 grid]$ export PATH=$PATH:$ORACLE_HOME/bin
[oracle@ora_02 grid]$ asmcmd
ASMCMD> lsdg
State    Type    Rebal  Sector  Logical_Sector  Block       AU  Total_MB  Free_MB  Req_mir_free_MB  Usable_file_MB  Offline_disks  Voting_files  Name
MOUNTED  EXTERN  N         512             512   4096  4194304     81920    81780                0           81780              0             N  DATA/
MOUNTED  EXTERN  N         512             512   4096  4194304     40960    40852                0           40852              0             N  LOGS/
ASMCMD> afd_state
ASMCMD-9526: The AFD state is 'LOADED' and filtering is 'ENABLED' on host 'ora_02'
ASMCMD> exit
[oracle@ora_02 grid]$

....
. Validar o status do serviço de HA.
+
....

[oracle@ora_02 bin]$ ./crsctl check has
CRS-4638: Oracle High Availability Services is online

....


====


=== Instalação do banco de dados Oracle

[%collapsible%open]
====
. Faça login como o usuário Oracle e desmarque `$ORACLE_HOME` e `$ORACLE_SID` se ele estiver definido.
+
[source, cli]
----
unset ORACLE_HOME
----
+
[source, cli]
----
unset ORACLE_SID
----
. Crie o diretório base do Oracle DB e altere o diretório para ele.
+
[source, cli]
----
mkdir /u01/app/oracle/product/19.0.0/cdb3
----
+
[source, cli]
----
cd /u01/app/oracle/product/19.0.0/cdb3
----
. Descompacte os arquivos de instalação do Oracle DB.
+
[source, cli]
----
unzip -q /tmp/archive/LINUX.X64_193000_db_home.zip
----
. A partir da base de dados, exclua o `OPatch` diretório.
+
[source, cli]
----
rm -rf OPatch
----
. A partir do DB home, descompacte . `p6880880_190000_Linux-x86-64.zip`
+
[source, cli]
----
unzip -q /tmp/archive/p6880880_190000_Linux-x86-64.zip
----
. A partir do banco de dados home, revise `cv/admin/cvu_config` e descomente e substitua `CV_ASSUME_DISTID=OEL5` por `CV_ASSUME_DISTID=OL7`.
+
[source, cli]
----
vi cv/admin/cvu_config
----
.  `/tmp/archive`No diretório, descompacte o patch DB 19,18 RU.
+
[source, cli]
----
unzip -q /tmp/archive/p34765931_190000_Linux-x86-64.zip -d /tmp/archive
----
. Prepare o arquivo DB Silent install rsp `/tmp/archive/dbinstall.rsp` no diretório com os seguintes valores:
+
....
oracle.install.option=INSTALL_DB_SWONLY
UNIX_GROUP_NAME=oinstall
INVENTORY_LOCATION=/u01/app/oraInventory
ORACLE_HOME=/u01/app/oracle/product/19.0.0/cdb3
ORACLE_BASE=/u01/app/oracle
oracle.install.db.InstallEdition=EE
oracle.install.db.OSDBA_GROUP=dba
oracle.install.db.OSOPER_GROUP=oper
oracle.install.db.OSBACKUPDBA_GROUP=oper
oracle.install.db.OSDGDBA_GROUP=dba
oracle.install.db.OSKMDBA_GROUP=dba
oracle.install.db.OSRACDBA_GROUP=dba
oracle.install.db.rootconfig.executeRootScript=false
....
. A partir de cdb3 home /u01/app/oracle/product/19,0.0/cdb3, execute a instalação silenciosa de banco de dados somente de software.
+
[source, cli]
----
 ./runInstaller -applyRU /tmp/archive/34765931/ -silent -ignorePrereqFailure -responseFile /tmp/archive/dbinstall.rsp
----
. Como usuário raiz, execute o `root.sh` script após a instalação somente de software.
+
[source, cli]
----
/u01/app/oracle/product/19.0.0/db1/root.sh
----
. Como usuário oracle, crie o `dbca.rsp` arquivo com as seguintes entradas:
+
....
gdbName=cdb3.demo.netapp.com
sid=cdb3
createAsContainerDatabase=true
numberOfPDBs=3
pdbName=cdb3_pdb
useLocalUndoForPDBs=true
pdbAdminPassword="yourPWD"
templateName=General_Purpose.dbc
sysPassword="yourPWD"
systemPassword="yourPWD"
dbsnmpPassword="yourPWD"
datafileDestination=+DATA
recoveryAreaDestination=+LOGS
storageType=ASM
diskGroupName=DATA
characterSet=AL32UTF8
nationalCharacterSet=AL16UTF16
listeners=LISTENER
databaseType=MULTIPURPOSE
automaticMemoryManagement=false
totalMemory=8192
....
. Como usuário oracle, inicie a criação de banco de dados com dbca.
+
[source, cli]
----
bin/dbca -silent -createDatabase -responseFile /tmp/archive/dbca.rsp
----
+
saída:



....

Prepare for db operation
7% complete
Registering database with Oracle Restart
11% complete
Copying database files
33% complete
Creating and starting Oracle instance
35% complete
38% complete
42% complete
45% complete
48% complete
Completing Database Creation
53% complete
55% complete
56% complete
Creating Pluggable Databases
60% complete
64% complete
69% complete
78% complete
Executing Post Configuration Actions
100% complete
Database creation complete. For details check the logfiles at:
 /u01/app/oracle/cfgtoollogs/dbca/cdb3.
Database Information:
Global Database Name:cdb3.vmc.netapp.com
System Identifier(SID):cdb3
Look at the log file "/u01/app/oracle/cfgtoollogs/dbca/cdb3/cdb3.log" for further details.

....
. Repita os mesmos procedimentos da etapa 2 para criar um banco de dados de contentores cdb4 em um ORACLE_HOME separado /u01/app/oracle/product/19,0.0/cdb4 com um único PDB.
. Como usuário Oracle, valide o Oracle Restart HA Services após a criação de banco de dados que todos os bancos de dados (cdb3, cdb4) estão registrados nos serviços de HA.
+
[source, cli]
----
/u01/app/oracle/product/19.0.0/grid/crsctl stat res -t
----
+
saída:

+
....

[oracle@ora_02 bin]$ ./crsctl stat res -t
--------------------------------------------------------------------------------
Name           Target  State        Server                   State details
--------------------------------------------------------------------------------
Local Resources
--------------------------------------------------------------------------------
ora.DATA.dg
               ONLINE  ONLINE       ora_02                   STABLE
ora.LISTENER.lsnr
               ONLINE  INTERMEDIATE ora_02                   Not All Endpoints Re
                                                             gistered,STABLE
ora.LOGS.dg
               ONLINE  ONLINE       ora_02                   STABLE
ora.asm
               ONLINE  ONLINE       ora_02                   Started,STABLE
ora.ons
               OFFLINE OFFLINE      ora_02                   STABLE
--------------------------------------------------------------------------------
Cluster Resources
--------------------------------------------------------------------------------
ora.cdb3.db
      1        ONLINE  ONLINE       ora_02                   Open,HOME=/u01/app/o
                                                             racle/product/19.0.0
                                                             /cdb3,STABLE
ora.cdb4.db
      1        ONLINE  ONLINE       ora_02                   Open,HOME=/u01/app/o
                                                             racle/product/19.0.0
                                                             /cdb4,STABLE
ora.cssd
      1        ONLINE  ONLINE       ora_02                   STABLE
ora.diskmon
      1        OFFLINE OFFLINE                               STABLE
ora.driver.afd
      1        ONLINE  ONLINE       ora_02                   STABLE
ora.evmd
      1        ONLINE  ONLINE       ora_02                   STABLE
--------------------------------------------------------------------------------
....
. Defina o usuário Oracle `.bash_profile` .
+
[source, cli]
----
vi ~/.bash_profile
----
+
Adicione as seguintes entradas:

+
....

export ORACLE_HOME=/u01/app/oracle/product/19.0.0/db3
export ORACLE_SID=db3
export PATH=$PATH:$ORACLE_HOME/bin
alias asm='export ORACLE_HOME=/u01/app/oracle/product/19.0.0/grid;export ORACLE_SID=+ASM;export PATH=$PATH:$ORACLE_HOME/bin'
alias cdb3='export ORACLE_HOME=/u01/app/oracle/product/19.0.0/cdb3;export ORACLE_SID=cdb3;export PATH=$PATH:$ORACLE_HOME/bin'
alias cdb4='export ORACLE_HOME=/u01/app/oracle/product/19.0.0/cdb4;export ORACLE_SID=cdb4;export PATH=$PATH:$ORACLE_HOME/bin'

....
. Valide o CDB/PDB criado para cdb3.
+
[source, cli]
----
cdb3
----
+
....

[oracle@ora_02 ~]$ sqlplus / as sysdba

SQL*Plus: Release 19.0.0.0.0 - Production on Mon Oct 9 08:19:20 2023
Version 19.18.0.0.0

Copyright (c) 1982, 2022, Oracle.  All rights reserved.


Connected to:
Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0

SQL> select name, open_mode from v$database;

NAME      OPEN_MODE
--------- --------------------
CDB3      READ WRITE

SQL> show pdbs

    CON_ID CON_NAME                       OPEN MODE  RESTRICTED
---------- ------------------------------ ---------- ----------
         2 PDB$SEED                       READ ONLY  NO
         3 CDB3_PDB1                      READ WRITE NO
         4 CDB3_PDB2                      READ WRITE NO
         5 CDB3_PDB3                      READ WRITE NO
SQL>

SQL> select name from v$datafile;

NAME
--------------------------------------------------------------------------------
+DATA/CDB3/DATAFILE/system.257.1149420273
+DATA/CDB3/DATAFILE/sysaux.258.1149420317
+DATA/CDB3/DATAFILE/undotbs1.259.1149420343
+DATA/CDB3/86B637B62FE07A65E053F706E80A27CA/DATAFILE/system.266.1149421085
+DATA/CDB3/86B637B62FE07A65E053F706E80A27CA/DATAFILE/sysaux.267.1149421085
+DATA/CDB3/DATAFILE/users.260.1149420343
+DATA/CDB3/86B637B62FE07A65E053F706E80A27CA/DATAFILE/undotbs1.268.1149421085
+DATA/CDB3/06FB206DF15ADEE8E065025056B66295/DATAFILE/system.272.1149422017
+DATA/CDB3/06FB206DF15ADEE8E065025056B66295/DATAFILE/sysaux.273.1149422017
+DATA/CDB3/06FB206DF15ADEE8E065025056B66295/DATAFILE/undotbs1.271.1149422017
+DATA/CDB3/06FB206DF15ADEE8E065025056B66295/DATAFILE/users.275.1149422033

NAME
--------------------------------------------------------------------------------
+DATA/CDB3/06FB21766256DF9AE065025056B66295/DATAFILE/system.277.1149422033
+DATA/CDB3/06FB21766256DF9AE065025056B66295/DATAFILE/sysaux.278.1149422033
+DATA/CDB3/06FB21766256DF9AE065025056B66295/DATAFILE/undotbs1.276.1149422033
+DATA/CDB3/06FB21766256DF9AE065025056B66295/DATAFILE/users.280.1149422049
+DATA/CDB3/06FB22629AC1DFD7E065025056B66295/DATAFILE/system.282.1149422049
+DATA/CDB3/06FB22629AC1DFD7E065025056B66295/DATAFILE/sysaux.283.1149422049
+DATA/CDB3/06FB22629AC1DFD7E065025056B66295/DATAFILE/undotbs1.281.1149422049
+DATA/CDB3/06FB22629AC1DFD7E065025056B66295/DATAFILE/users.285.1149422063

19 rows selected.

SQL>

....
. Valide o CDB/PDB criado para cdb4.
+
[source, cli]
----
cdb4
----
+
....

[oracle@ora_02 ~]$ sqlplus / as sysdba

SQL*Plus: Release 19.0.0.0.0 - Production on Mon Oct 9 08:20:26 2023
Version 19.18.0.0.0

Copyright (c) 1982, 2022, Oracle.  All rights reserved.


Connected to:
Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0

SQL> select name, open_mode from v$database;

NAME      OPEN_MODE
--------- --------------------
CDB4      READ WRITE

SQL> show pdbs

    CON_ID CON_NAME                       OPEN MODE  RESTRICTED
---------- ------------------------------ ---------- ----------
         2 PDB$SEED                       READ ONLY  NO
         3 CDB4_PDB                       READ WRITE NO
SQL>

SQL> select name from v$datafile;

NAME
--------------------------------------------------------------------------------
+DATA/CDB4/DATAFILE/system.286.1149424943
+DATA/CDB4/DATAFILE/sysaux.287.1149424989
+DATA/CDB4/DATAFILE/undotbs1.288.1149425015
+DATA/CDB4/86B637B62FE07A65E053F706E80A27CA/DATAFILE/system.295.1149425765
+DATA/CDB4/86B637B62FE07A65E053F706E80A27CA/DATAFILE/sysaux.296.1149425765
+DATA/CDB4/DATAFILE/users.289.1149425015
+DATA/CDB4/86B637B62FE07A65E053F706E80A27CA/DATAFILE/undotbs1.297.1149425765
+DATA/CDB4/06FC3070D5E12C23E065025056B66295/DATAFILE/system.301.1149426581
+DATA/CDB4/06FC3070D5E12C23E065025056B66295/DATAFILE/sysaux.302.1149426581
+DATA/CDB4/06FC3070D5E12C23E065025056B66295/DATAFILE/undotbs1.300.1149426581
+DATA/CDB4/06FC3070D5E12C23E065025056B66295/DATAFILE/users.304.1149426597

11 rows selected.

....
. Faça login em cada cdb como sysdba com sqlplus e defina o tamanho do destino de recuperação de banco de dados para o tamanho do grupo de discos de LOGS DE
+
[source, cli]
----
alter system set db_recovery_file_dest_size = 40G scope=both;
----
. Inicie sessão em cada cdb como sysdba com sqlplus e ative o modo de registo de arquivo com os seguintes conjuntos de comandos em sequência.
+
[source, cli]
----
sqlplus /as sysdba
----
+
[source, cli]
----
shutdown immediate;
----
+
[source, cli]
----
startup mount;
----
+
[source, cli]
----
alter database archivelog;
----
+
[source, cli]
----
alter database open;
----


Isso conclui a implantação de reinicialização do Oracle 19C versão 19,18 em um armazenamento do Amazon FSX ONTAP e uma VM de banco de dados VMC. Se desejado, o NetApp recomenda a realocação do arquivo de controle Oracle e dos arquivos de log on-line para o grupo de discos DE LOGS.

====


=== Faça backup, restauração e clone do Oracle com o SnapCenter



==== Configuração do SnapCenter

[%collapsible%open]
====
O SnapCenter confia em um plug-in do lado do host na VM do banco de dados para executar atividades de gerenciamento de proteção de dados com reconhecimento de aplicações. Para obter informações detalhadas sobre o plugin NetApp SnapCenter para Oracle, consulte esta documentação link:https://docs.netapp.com/us-en/snapcenter/protect-sco/concept_what_you_can_do_with_the_snapcenter_plug_in_for_oracle_database.html["O que você pode fazer com o Plug-in para Oracle Database"^]. A seguir, fornece etapas de alto nível para configurar o SnapCenter para backup, recuperação e clone de banco de dados Oracle.

. Baixe a versão mais recente do software SnapCenter no site de suporte da NetApp: link:https://mysupport.netapp.com/site/downloads["Downloads de suporte da NetApp"^].
. Como administrador, instale o JDK java mais recente a partir do link:https://www.java.com/en/["Obtenha o Java para aplicativos de desktop"^]host Windows do servidor SnapCenter.
+

NOTE: Se o servidor Windows for implantado em um ambiente de domínio, adicione um usuário de domínio ao grupo de administradores locais do servidor SnapCenter e execute a instalação do SnapCenter com o usuário do domínio.

. Faça login na IU do SnapCenter via HTTPS porta 8846 como usuário de instalação para configurar o SnapCenter para Oracle.
. Atualização `Hypervisor Settings` nas definições globais.
+
image:aws_ora_fsx_vmc_snapctr_01.png["Captura de tela mostrando a configuração do SnapCenter."]

. Criar políticas de backup de banco de dados Oracle. Idealmente, crie uma política de backup de log de arquivamento separada para permitir um intervalo de backup mais frequente para minimizar a perda de dados em caso de falha.
+
image:aws_ora_fsx_vmc_snapctr_02.png["Captura de tela mostrando a configuração do SnapCenter."]

. Adicionar servidor de banco de dados para acesso SnapCenter à VM de banco de dados `Credential`. A credencial deve ter privilégio sudo em uma VM Linux ou privilégio de administrador em uma VM Windows.
+
image:aws_ora_fsx_vmc_snapctr_03.png["Captura de tela mostrando a configuração do SnapCenter."]

. Adicione o cluster de armazenamento FSX ONTAP `Storage Systems` com IP de gerenciamento de cluster e autenticado via fsxadmin ID de usuário.
+
image:aws_ora_fsx_vmc_snapctr_04.png["Captura de tela mostrando a configuração do SnapCenter."]

. Adicione VM do banco de dados Oracle no VMC `Hosts` com credencial do servidor criada na etapa anterior 6.
+
image:aws_ora_fsx_vmc_snapctr_05.png["Captura de tela mostrando a configuração do SnapCenter."]




NOTE: Certifique-se de que o nome do servidor SnapCenter possa ser resolvido para o endereço IP da VM da DB e o nome da VM da DB pode ser resolvido para o endereço IP do servidor SnapCenter.

====


==== Backup de banco de dados

[%collapsible%open]
====
O SnapCenter aproveita o snapshot de volume do FSX ONTAP para backup, restauração ou clone de banco de dados muito mais rápidos em comparação com a metodologia tradicional baseada em RMAN. Os snapshots são consistentes com aplicações, pois o banco de dados é colocado no modo de backup Oracle antes de um snapshot.

. Na `Resources` guia, quaisquer bancos de dados na VM são automaticamente descobertos depois que a VM é adicionada ao SnapCenter. Inicialmente, o status do banco de dados é exibido como `Not protected`.
+
image:aws_ora_fsx_vmc_snapctr_06.png["Captura de tela mostrando a configuração do SnapCenter."]

. Criar um grupo de recursos para fazer backup do banco de dados em um agrupamento lógico, como por VM de banco de dados, etc. neste exemplo, criamos um grupo ora_02_data para fazer um backup completo de banco de dados on-line para todos os bancos de dados na VM ora_02. O grupo de recursos ora_02_log executa o backup de logs arquivados somente na VM. A criação de um grupo de recursos também define uma programação para executar o backup.
+
image:aws_ora_fsx_vmc_snapctr_07.png["Captura de tela mostrando a configuração do SnapCenter."]

. O backup do grupo de recursos também pode ser acionado manualmente clicando `Back up Now` e executando o backup com a política definida no grupo recursos.
+
image:aws_ora_fsx_vmc_snapctr_08.png["Captura de tela mostrando a configuração do SnapCenter."]

. O trabalho de cópia de segurança pode ser monitorizado `Monitor` no separador clicando no trabalho em execução.
+
image:aws_ora_fsx_vmc_snapctr_09.png["Captura de tela mostrando a configuração do SnapCenter."]

. Após uma cópia de segurança bem-sucedida, o estado da base de dados mostra o estado da tarefa e o tempo de cópia de segurança mais recente.
+
image:aws_ora_fsx_vmc_snapctr_10.png["Captura de tela mostrando a configuração do SnapCenter."]

. Clique no banco de dados para revisar os conjuntos de backup de cada banco de dados.
+
image:aws_ora_fsx_vmc_snapctr_11.png["Captura de tela mostrando a configuração do SnapCenter."]



====


==== Recuperação de banco de dados

[%collapsible%open]
====
O SnapCenter fornece várias opções de restauração e recuperação para bancos de dados Oracle a partir do backup instantâneo. Neste exemplo, demonstramos um ponto no tempo de restauração para recuperar uma tabela descartada por engano. Na VM ora_02, dois bancos de dados cdb3, cdb4 compartilham os mesmos grupos de disco de DADOS e LOGS DE DADOS. A restauração de banco de dados para um banco de dados não afeta a disponibilidade do outro banco de dados.

. Primeiro, crie uma tabela de teste e insira uma linha na tabela para validar um ponto na recuperação de tempo.
+
....

[oracle@ora_02 ~]$ sqlplus / as sysdba

SQL*Plus: Release 19.0.0.0.0 - Production on Fri Oct 6 14:15:21 2023
Version 19.18.0.0.0

Copyright (c) 1982, 2022, Oracle.  All rights reserved.


Connected to:
Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0

SQL> select name, open_mode from v$database;

NAME      OPEN_MODE
--------- --------------------
CDB3      READ WRITE

SQL> show pdbs

    CON_ID CON_NAME                       OPEN MODE  RESTRICTED
---------- ------------------------------ ---------- ----------
         2 PDB$SEED                       READ ONLY  NO
         3 CDB3_PDB1                      READ WRITE NO
         4 CDB3_PDB2                      READ WRITE NO
         5 CDB3_PDB3                      READ WRITE NO
SQL>


SQL> alter session set container=cdb3_pdb1;

Session altered.

SQL> create table test (id integer, dt timestamp, event varchar(100));

Table created.

SQL> insert into test values(1, sysdate, 'test oracle recovery on guest mounted fsx storage to VMC guest vm ora_02');

1 row created.

SQL> commit;

Commit complete.

SQL> select * from test;

        ID
----------
DT
---------------------------------------------------------------------------
EVENT
--------------------------------------------------------------------------------
         1
06-OCT-23 03.18.24.000000 PM
test oracle recovery on guest mounted fsx storage to VMC guest vm ora_02


SQL> select current_timestamp from dual;

CURRENT_TIMESTAMP
---------------------------------------------------------------------------
06-OCT-23 03.18.53.996678 PM -07:00

....
. Executamos um backup instantâneo manual do SnapCenter. Em seguida, solte a tabela.
+
....

SQL> drop table test;

Table dropped.

SQL> commit;

Commit complete.

SQL> select current_timestamp from dual;

CURRENT_TIMESTAMP
---------------------------------------------------------------------------
06-OCT-23 03.26.30.169456 PM -07:00

SQL> select * from test;
select * from test
              *
ERROR at line 1:
ORA-00942: table or view does not exist

....
. A partir do conjunto de cópias de segurança criado a partir da última etapa, anote o número de cópias de segurança do registo SCN. Clique `Restore` em para iniciar o fluxo de trabalho de restauração-recuperação.
+
image:aws_ora_fsx_vmc_snapctr_12.png["Captura de tela mostrando a configuração do SnapCenter."]

. Escolha restaurar escopo.
+
image:aws_ora_fsx_vmc_snapctr_13.png["Captura de tela mostrando a configuração do SnapCenter."]

. Escolha o escopo de recuperação até o SCN de log do último backup completo do banco de dados.
+
image:aws_ora_fsx_vmc_snapctr_14.png["Captura de tela mostrando a configuração do SnapCenter."]

. Especifique quaisquer pré-scripts opcionais a serem executados.
+
image:aws_ora_fsx_vmc_snapctr_15.png["Captura de tela mostrando a configuração do SnapCenter."]

. Especifique qualquer pós-script opcional a ser executado.
+
image:aws_ora_fsx_vmc_snapctr_16.png["Captura de tela mostrando a configuração do SnapCenter."]

. Envie um relatório de trabalho, se desejado.
+
image:aws_ora_fsx_vmc_snapctr_17.png["Captura de tela mostrando a configuração do SnapCenter."]

. Reveja o resumo e clique `Finish` em para iniciar a restauração e recuperação.
+
image:aws_ora_fsx_vmc_snapctr_18.png["Captura de tela mostrando a configuração do SnapCenter."]

. A partir do Oracle Restart Grid control, observamos que enquanto cdb3 está em restauração e recuperação cdb4 está on-line e disponível.
+
image:aws_ora_fsx_vmc_snapctr_19.png["Captura de tela mostrando a configuração do SnapCenter."]

. No `Monitor` separador , abra o trabalho para rever os detalhes.
+
image:aws_ora_fsx_vmc_snapctr_20.png["Captura de tela mostrando a configuração do SnapCenter."]

. A partir de DB VM ora_02, valide que a tabela descartada seja recuperada após uma recuperação bem-sucedida.
+
....

[oracle@ora_02 bin]$ sqlplus / as sysdba

SQL*Plus: Release 19.0.0.0.0 - Production on Fri Oct 6 17:01:28 2023
Version 19.18.0.0.0

Copyright (c) 1982, 2022, Oracle.  All rights reserved.


Connected to:
Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0

SQL> select name, open_mode from v$database;

NAME      OPEN_MODE
--------- --------------------
CDB3      READ WRITE

SQL> show pdbs

    CON_ID CON_NAME                       OPEN MODE  RESTRICTED
---------- ------------------------------ ---------- ----------
         2 PDB$SEED                       READ ONLY  NO
         3 CDB3_PDB1                      READ WRITE NO
         4 CDB3_PDB2                      READ WRITE NO
         5 CDB3_PDB3                      READ WRITE NO
SQL> alter session set container=CDB3_PDB1;

Session altered.

SQL> select * from test;

        ID
----------
DT
---------------------------------------------------------------------------
EVENT
--------------------------------------------------------------------------------
         1
06-OCT-23 03.18.24.000000 PM
test oracle recovery on guest mounted fsx storage to VMC guest vm ora_02


SQL> select current_timestamp from dual;

CURRENT_TIMESTAMP
---------------------------------------------------------------------------
06-OCT-23 05.02.20.382702 PM -07:00

SQL>

....


====


==== Clone de banco de dados

[%collapsible%open]
====
Neste exemplo, os mesmos conjuntos de backup são usados para clonar um banco de dados na mesma VM em um ORACLE_HOME diferente. Os procedimentos são igualmente aplicáveis para clonar um banco de dados do backup para separar a VM no VMC, se necessário.

. Abra a lista de backup do banco de dados cdb3. A partir de um backup de dados escolhido, clique `Clone` no botão para iniciar o fluxo de trabalho clone de banco de dados.
+
image:aws_ora_fsx_vmc_snapctr_21.png["Captura de tela mostrando a configuração do SnapCenter."]

. Nomeie o SID do banco de dados clone.
+
image:aws_ora_fsx_vmc_snapctr_22.png["Captura de tela mostrando a configuração do SnapCenter."]

. Selecione uma VM no VMC como o host do banco de dados de destino. A versão Oracle idêntica deve ter sido instalada e configurada no host.
+
image:aws_ora_fsx_vmc_snapctr_23.png["Captura de tela mostrando a configuração do SnapCenter."]

. Selecione o Oracle_HOME, o usuário e o grupo apropriados no host de destino. Mantenha a credencial como padrão.
+
image:aws_ora_fsx_vmc_snapctr_24.png["Captura de tela mostrando a configuração do SnapCenter."]

. Altere os parâmetros do banco de dados clone para atender aos requisitos de configuração ou recursos do banco de dados clone.
+
image:aws_ora_fsx_vmc_snapctr_25.png["Captura de tela mostrando a configuração do SnapCenter."]

. Escolha o escopo de recuperação. `Until Cancel` recupera o clone até o último arquivo de log disponível no conjunto de backup.
+
image:aws_ora_fsx_vmc_snapctr_26.png["Captura de tela mostrando a configuração do SnapCenter."]

. Revise o resumo e inicie o trabalho clone.
+
image:aws_ora_fsx_vmc_snapctr_27.png["Captura de tela mostrando a configuração do SnapCenter."]

. Monitore a execução da tarefa clone a partir `Monitor` da guia.
+
image:aws_ora_fsx_vmc_snapctr_28.png["Captura de tela mostrando a configuração do SnapCenter."]

. O banco de dados clonado é imediatamente registrado no SnapCenter.
+
image:aws_ora_fsx_vmc_snapctr_29.png["Captura de tela mostrando a configuração do SnapCenter."]

. A partir do DB VM ora_02, o banco de dados clonado também é registrado no controle de grade de reinicialização do Oracle e a tabela de teste descartada é recuperada no banco de dados clonado cdb3tst, como mostrado abaixo.
+
....

[oracle@ora_02 ~]$ /u01/app/oracle/product/19.0.0/grid/bin/crsctl stat res -t
--------------------------------------------------------------------------------
Name           Target  State        Server                   State details
--------------------------------------------------------------------------------
Local Resources
--------------------------------------------------------------------------------
ora.DATA.dg
               ONLINE  ONLINE       ora_02                   STABLE
ora.LISTENER.lsnr
               ONLINE  INTERMEDIATE ora_02                   Not All Endpoints Re
                                                             gistered,STABLE
ora.LOGS.dg
               ONLINE  ONLINE       ora_02                   STABLE
ora.SC_2090922_CDB3TST.dg
               ONLINE  ONLINE       ora_02                   STABLE
ora.asm
               ONLINE  ONLINE       ora_02                   Started,STABLE
ora.ons
               OFFLINE OFFLINE      ora_02                   STABLE
--------------------------------------------------------------------------------
Cluster Resources
--------------------------------------------------------------------------------
ora.cdb3.db
      1        ONLINE  ONLINE       ora_02                   Open,HOME=/u01/app/o
                                                             racle/product/19.0.0
                                                             /cdb3,STABLE
ora.cdb3tst.db
      1        ONLINE  ONLINE       ora_02                   Open,HOME=/u01/app/o
                                                             racle/product/19.0.0
                                                             /cdb4,STABLE
ora.cdb4.db
      1        ONLINE  ONLINE       ora_02                   Open,HOME=/u01/app/o
                                                             racle/product/19.0.0
                                                             /cdb4,STABLE
ora.cssd
      1        ONLINE  ONLINE       ora_02                   STABLE
ora.diskmon
      1        OFFLINE OFFLINE                               STABLE
ora.driver.afd
      1        ONLINE  ONLINE       ora_02                   STABLE
ora.evmd
      1        ONLINE  ONLINE       ora_02                   STABLE
--------------------------------------------------------------------------------

[oracle@ora_02 ~]$ export ORACLE_HOME=/u01/app/oracle/product/19.0.0/cdb4
[oracle@ora_02 ~]$ export ORACLE_SID=cdb3tst
[oracle@ora_02 ~]$ sqlplus / as sysdba

SQL*Plus: Release 19.0.0.0.0 - Production on Sat Oct 7 08:04:51 2023
Version 19.18.0.0.0

Copyright (c) 1982, 2022, Oracle.  All rights reserved.


Connected to:
Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production
Version 19.18.0.0.0

SQL> select name, open_mode from v$database;

NAME      OPEN_MODE
--------- --------------------
CDB3TST   READ WRITE

SQL> show pdbs

    CON_ID CON_NAME                       OPEN MODE  RESTRICTED
---------- ------------------------------ ---------- ----------
         2 PDB$SEED                       READ ONLY  NO
         3 CDB3_PDB1                      READ WRITE NO
         4 CDB3_PDB2                      READ WRITE NO
         5 CDB3_PDB3                      READ WRITE NO
SQL> alter session set container=CDB3_PDB1;

Session altered.

SQL> select * from test;

        ID
----------
DT
---------------------------------------------------------------------------
EVENT
--------------------------------------------------------------------------------
         1
06-OCT-23 03.18.24.000000 PM
test oracle recovery on guest mounted fsx storage to VMC guest vm ora_02


SQL>

....


Isso conclui a demonstração do backup, restauração e clone do SnapCenter do banco de dados Oracle no VMC SDDC na AWS.

====


== Onde encontrar informações adicionais

Para saber mais sobre as informações descritas neste documento, consulte os seguintes documentos e/ou sites:

* Documentação do VMware Cloud na AWS
+
link:https://docs.vmware.com/en/VMware-Cloud-on-AWS/index.html["https://docs.vmware.com/en/VMware-Cloud-on-AWS/index.html"^]

* Instalando o Oracle Grid Infrastructure para um servidor autônomo com uma nova instalação de banco de dados
+
link:https://docs.oracle.com/en/database/oracle/oracle-database/19/ladbi/installing-oracle-grid-infrastructure-for-a-standalone-server-with-a-new-database-installation.html#GUID-0B1CEE8C-C893-46AA-8A6A-7B5FAAEC72B3["https://docs.oracle.com/en/database/oracle/oracle-database/19/ladbi/installing-oracle-grid-infrastructure-for-a-standalone-server-with-a-new-database-installation.html#GUID-0B1CEE8C-C893-46AA-8A6A-7B5FAAEC72B3"^]

* Instalando e configurando o banco de dados Oracle usando arquivos de resposta
+
link:https://docs.oracle.com/en/database/oracle/oracle-database/19/ladbi/installing-and-configuring-oracle-database-using-response-files.html#GUID-D53355E9-E901-4224-9A2A-B882070EDDF7["https://docs.oracle.com/en/database/oracle/oracle-database/19/ladbi/installing-and-configuring-oracle-database-using-response-files.html#GUID-D53355E9-E901-4224-9A2A-B882070EDDF7"^]

* Amazon FSX ONTAP
+
link:https://aws.amazon.com/fsx/netapp-ontap/["https://aws.amazon.com/fsx/netapp-ontap/"^]


